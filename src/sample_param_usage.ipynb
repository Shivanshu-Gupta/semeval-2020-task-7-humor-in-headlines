{
 "cells": [
  {
   "source": [
    "# Attr-Parameters\n",
    "This notebook shows how to use `Parameters` which is a dataclass developed using the `attr` package along with all the additional features it provides."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b0c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import attr\n",
    "from pprint import pprint as print\n",
    "from typing import Union, Optional\n",
    "from copy import deepcopy"
   ]
  },
  {
   "source": [
    "The following need to be imported from `param_impl` module to get full benefit of this framework:\n",
    "\n",
    "1.  `Parameters`: This (data)class forms the core of the framework. All param classes should subclass this (and additionally add the `@attr.s(auto_atribs=True)` decorator).\n",
    "2.  `Settings`: A list-type class used to specify multiple values for a parameter for hyper-parameter search. Supports all operations of a regular python `list`.\n",
    "3.  `default_value`: A function used specify default values when they are mutable eg. list, class objects etc. Refer to [this](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments) to know why this is important."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from param_impl import Parameters, Settings, default_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb75e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate(o, t): \n",
    "    lambdas = {\n",
    "        Union[AdamOptimizerParams, SGDOptimizerParams]: lambda o, _: SGDOptimizerParams if 'momentum' in o else AdamOptimizerParams,\n",
    "        Union[int, str]: lambda *_: None\n",
    "    }\n",
    "    if t in lambdas:\n",
    "        return lambdas[t](o, t)\n",
    "    # elif t == Union[t1, t2, t3]:  # Write disambiguator like this when a simple lambda is not possible\n",
    "    #     pass\n",
    "    else:\n",
    "        raise TypeError(\"Unknown Type\")\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class EncoderParams(Parameters):\n",
    "    type: str = 'torch.nn.LSTM'\n",
    "    hidden_size: int = 100\n",
    "    num_layers: int = 1\n",
    "    bias: bool = True\n",
    "    dropout: float = 0\n",
    "    bidirectional: bool = True\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class ModelParams(Parameters):\n",
    "    type: str = 'models.simple_tagger.SimpleTagger'\n",
    "    embedding_param: Union[int, str] = 50\n",
    "    encoder: Optional[EncoderParams] = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_disambiguators(cls):\n",
    "        return {Union[int, str]: disambiguate}\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class AdamOptimizerParams(Parameters):\n",
    "    type: str = 'torch.optim.Adam'\n",
    "    lr: float = 0.001\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class SGDOptimizerParams(Parameters):\n",
    "    type: str = 'torch.optim.SGD'\n",
    "    lr: float = 0.001\n",
    "    momentum: float = 0.1\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class TrainingParams(Parameters):\n",
    "    num_epochs: int = 20\n",
    "    optimizer: Union[AdamOptimizerParams,\n",
    "                     SGDOptimizerParams] = default_value(AdamOptimizerParams())\n",
    "\n",
    "    @classmethod\n",
    "    def get_disambiguators(cls):\n",
    "        return {Union[AdamOptimizerParams, SGDOptimizerParams]: disambiguate}\n",
    "\n",
    "\n",
    "@attr.s(auto_attribs=True)\n",
    "class TaggingParams(Parameters):\n",
    "    random_seed: int = 42\n",
    "    gpu_idx: int = -1\n",
    "    model: ModelParams = default_value(ModelParams())\n",
    "    training: TrainingParams = default_value(TrainingParams())\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        # this function is called by attr after __init__()\n",
    "        # useful to modify default values\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0f30e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='models.simple_tagger.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001)))\n"
     ]
    }
   ],
   "source": [
    "params = TaggingParams()\n",
    "print(params)"
   ]
  },
  {
   "source": [
    "## Dictionary\n",
    "`Parameters` can be easily converted to and from dicts as well as flattened dicts. The latter is useful because many packages (eg. comet_ml) do not support nested configurations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3255e621",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': None,\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': 0.001, 'type': 'torch.optim.Adam'}}}\nTaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='models.simple_tagger.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001)))\n"
     ]
    }
   ],
   "source": [
    "# easy conversion to and from dict\n",
    "print(params.to_dict())\n",
    "print(TaggingParams.from_dict(params.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48b9ba01",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder': None,\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.001,\n 'training.optimizer.type': 'torch.optim.Adam'}\nTaggingParams(random_seed=42, gpu_idx=-1, model=ModelParams(type='models.simple_tagger.SimpleTagger', embedding_param=50, encoder=None), training=TrainingParams(num_epochs=20, optimizer=AdamOptimizerParams(type='torch.optim.Adam', lr=0.001)))\n"
     ]
    }
   ],
   "source": [
    "# easy conversion to and from flattend dict\n",
    "print(params.to_flattened_dict())\n",
    "print(TaggingParams.from_flattened_dict(params.to_flattened_dict()))"
   ]
  },
  {
   "source": [
    "Equality comparison is supported out of the box (thanks to `attr`). So easy to check desirialising from dictionaries gives the same parameters:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "888da7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TaggingParams.from_dict(params.to_dict()) == params\n",
    "assert TaggingParams.from_flattened_dict(params.to_flattened_dict()) == params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'embedding_param': 50,\n 'encoder': None,\n 'type': 'models.simple_tagger.SimpleTagger'}\n{'embedding_param': 50,\n 'encoder': None,\n 'type': 'models.simple_tagger.SimpleTagger'}\n"
     ]
    }
   ],
   "source": [
    "# Both dict-like and attribute access are supported:\n",
    "print(params.model.to_dict())\n",
    "print(params['model'].to_dict())\n",
    "assert params.model == params['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 100,\n           'encoder': {'bias': True,\n                       'bidirectional': True,\n                       'dropout': 0,\n                       'hidden_size': 100,\n                       'num_layers': 1,\n                       'type': 'torch.nn.LSTM'},\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': 0.001, 'type': 'torch.optim.Adam'}}}\n{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': None,\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': 0.001, 'type': 'torch.optim.Adam'}}}\n"
     ]
    }
   ],
   "source": [
    "# can modify using both dict and attribute access\n",
    "_params = deepcopy(params)\n",
    "_params.model.encoder = EncoderParams()\n",
    "_params['model']['embedding_param'] = 100\n",
    "print(_params.to_dict())\n",
    "print(params.to_dict())"
   ]
  },
  {
   "source": [
    "## Hyper-parameter Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Directly using `Parameters`\n",
    "`Parameters` can be directly used to specify the values to try out for each parameter and then to get all settings in the grid formed by product of values for each parameter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': {'bias': True,\n                       'bidirectional': True,\n                       'dropout': 0,\n                       'hidden_size': 100,\n                       'num_layers': 1,\n                       'type': 'torch.nn.LSTM'},\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': 0.001, 'type': 'torch.optim.Adam'}}}\n"
     ]
    }
   ],
   "source": [
    "params = TaggingParams(model=ModelParams(encoder=EncoderParams()))\n",
    "print(params.to_dict())"
   ]
  },
  {
   "source": [
    "Use `Settings` to specify different values for each parameter:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.model.encoder.hidden_size = Settings([50, 100])\n",
    "params.training.optimizer.lr = Settings([1e-2, 1e-1])"
   ]
  },
  {
   "source": [
    "Now just use the `get_settings()` function to get all the different possible settings:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 50,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.01,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 50,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.1,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 100,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.01,\n 'training.optimizer.type': 'torch.optim.Adam'}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': 100,\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': 0.1,\n 'training.optimizer.type': 'torch.optim.Adam'}\n"
     ]
    }
   ],
   "source": [
    "settings = params.get_settings()\n",
    "print(len(settings))        # will be equal to the product of the number of values for each parameter\n",
    "for setting in settings:\n",
    "    print(setting.to_flattened_dict())"
   ]
  },
  {
   "source": [
    "### Using Raytune without Search Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'gpu_idx': -1,\n 'model': {'embedding_param': 50,\n           'encoder': {'bias': True,\n                       'bidirectional': True,\n                       'dropout': 0,\n                       'hidden_size': 100,\n                       'num_layers': 1,\n                       'type': 'torch.nn.LSTM'},\n           'type': 'models.simple_tagger.SimpleTagger'},\n 'random_seed': 42,\n 'training': {'num_epochs': 20,\n              'optimizer': {'lr': [0.01, 0.1], 'type': 'torch.optim.Adam'}}}\n{'gpu_idx': -1,\n 'model.embedding_param': 50,\n 'model.encoder.bias': True,\n 'model.encoder.bidirectional': True,\n 'model.encoder.dropout': 0,\n 'model.encoder.hidden_size': {'grid_search': [50, 100]},\n 'model.encoder.num_layers': 1,\n 'model.encoder.type': 'torch.nn.LSTM',\n 'model.type': 'models.simple_tagger.SimpleTagger',\n 'random_seed': 42,\n 'training.num_epochs': 20,\n 'training.optimizer.lr': <ray.tune.sample.Float object at 0x7f9d9116c370>,\n 'training.optimizer.type': 'torch.optim.Adam'}\n"
     ]
    }
   ],
   "source": [
    "params = TaggingParams(model=ModelParams(encoder=EncoderParams()))\n",
    "print(params.to_dict())\n",
    "params.model.encoder.hidden_size = tune.grid_search([50, 100])\n",
    "params.training.optimizer.lr = tune.loguniform(1e-3, 1e-1)\n",
    "print(params.to_flattened_dict())\n",
    "# Now just pass `params.to_flattened_dict()` as `config` parameter to `tune.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd00ae59ccd0a25b34c1138f46f30ecc7218c820f706af420d3d408be82c81b9c7c",
   "display_name": "Python 3.8.8 64-bit ('dl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "0ae59ccd0a25b34c1138f46f30ecc7218c820f706af420d3d408be82c81b9c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}